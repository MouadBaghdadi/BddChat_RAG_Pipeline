{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858501def67b4d699fb24d88cbbbadcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "#                                          bnb_4bit_compute_dtype=torch.float16)\n",
    "model_id = 'google/gemma-2b-it' \n",
    "print(f\"model_id: {model_id}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.bfloat16) # which attention version to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506172416"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 5012354048, 'model_mem_mb': 4780.15, 'model_mem_gb': 4.67}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) \n",
    "    model_mem_gb = model_mem_bytes / (1024**3) \n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "explique moi le schéma relationnel?\n",
      "\n",
      "Prompt (formatted):\n",
      "<bos><start_of_turn>user\n",
      "explique moi le schéma relationnel?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"explique moi le schéma relationnel?\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "#craete prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "#apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep it not tokenized\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 21s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inputs = tokenizer.encode(prompt, \n",
    "                      add_special_tokens= True,\n",
    "                      return_tensors=\"pt\" )\n",
    "\n",
    "outputs = llm_model.generate(input_ids=inputs.to(llm_model.device), max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     2,      2,    106,   1645,    108, 214032,  19653,    709, 190251,\n",
       "         10189,   2408, 235336,    107,    108,    106,   2516,    108,    688,\n",
       "          7758,  36511,  10189,   2408,    688,    109,   2331, 190251,  10189,\n",
       "          2408,   1455,   2360, 142772, 108088,   2459,  52324,   1437,   4106,\n",
       "          3692,   1437,  48559,    499, 235303,    549,  26889, 235265,   4626,\n",
       "         40357,   1437,  48559,    581,    533, 235303,  34853,   2499,    848,\n",
       "        205633,   1008,   1437,   4106,   3692,  34092,   2499,    848,  70244,\n",
       "        235265,    109,    688,  16053,    499, 235303,    549, 190251,  10189,\n",
       "          2408,    865,    688,    109, 235287,   5231,   1836,  22181,    865,\n",
       "           688,   5221,  48559,    581,    533, 235303,  34853, 235265,    108,\n",
       "        235287,   5231,   5479,  41955,    865,    688,   2481,   4106,   3692,\n",
       "          1437,  66262, 235265,    108, 235287,   5231,   4965, 123843,    865,\n",
       "           688,   2481, 225592,   2459, 156268,   1437,   4106, 235265,    108,\n",
       "        235287,   5231,   4007, 235688,   2231,    865,    688,   2481, 205633,\n",
       "          2459, 156268,   1437,  66262, 235265,    109,    688,   6882,    581,\n",
       "          4106,    865,    688,    109, 235287,   5231,  44935,    499, 235303,\n",
       "        188929,    865,    688,   5297,    586,   1455,  60137,   2499,    599,\n",
       "        235269,  18956,  72828,    907,    586,   8656,  19894,    581,    599,\n",
       "        235265,    108, 235287,   5231,  44935])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><bos><start_of_turn>user\\nexplique moi le schéma relationnel?<end_of_turn>\\n<start_of_turn>model\\n**Schéma relationnel**\\n\\nUn schéma relationnel est une représentation graphique qui montre les relations entre les éléments d'un ensemble. Il présente les éléments de l'ensemble dans des boîtes et les relations entre eux dans des lignes.\\n\\n**Components d'un schéma relationnel :**\\n\\n* **Objets :** Les éléments de l'ensemble.\\n* **Relationen :** Des relations entre les objets.\\n* **Connecteurs :** Des symboles qui représentent les relations.\\n* **Boîtes :** Des boîtes qui représentent les objets.\\n\\n**Types de relations :**\\n\\n* **Relation d'inclusion :** Si A est inclus dans B, cela signifie que A fait partie de B.\\n* **Relation\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the output tokens into readable text\n",
    "\n",
    "output_decoded = tokenizer.decode(outputs[0])\n",
    "output_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model\" \n",
    "llm_model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
