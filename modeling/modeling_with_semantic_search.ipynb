{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embeddings = pd.read_csv('C:\\\\Users\\\\hp probook\\\\Desktop\\\\RAG_Project\\\\data_preprocessing\\\\bdd.csv')\n",
    "\n",
    "text_chunks_and_embeddings['embeddings'] = text_chunks_and_embeddings['embeddings'].apply(lambda x : np.fromstring(x.strip(\"[]\"), sep=', '))\n",
    "\n",
    "# text_chunks_and_embeddings['embeddings'] = text_chunks_and_embeddings['embeddings'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "\n",
    "pages_and_chunks = text_chunks_and_embeddings.to_dict(orient= \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([165, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.tensor(np.stack(text_chunks_and_embeddings['embeddings'].tolist(), axis=0))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device= device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steps:\n",
    "* define a query string \n",
    "* turn the query into an embedding vector \n",
    "* perform the similarity search by using teh cosine similarity function between the chunks embedding and the query embedding\n",
    "* choose the 5 chunks with highest degree of similarity   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfaiss library to create vector database\\nand ann methode to seacrh the relevent passages \\ncosine similarity is just the dot product of values already normalized (L2 norm : la racine de (la somme des ( valeurs au carré))), and it's favored to use cosine similarity then the dot product when we do semantic search\\n\\nmixedbread rerank model used to rerank the top k values \\nfalshattention to speed up the attention mechanism of transformers and generating the token of our llm\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "faiss library to create vector database\n",
    "and ann methode to seacrh the relevent passages \n",
    "cosine similarity is just the dot product of values already normalized (L2 norm : la racine de (la somme des ( valeurs au carré))), and it's favored to use cosine similarity then the dot product when we do semantic search\n",
    "\n",
    "mixedbread rerank model used to rerank the top k values \n",
    "falshattention to speed up the attention mechanism of transformers and generating the token of our llm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: explique la théorie de la normalisation\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.7604, 0.5894, 0.5825, 0.5562, 0.5499]),\n",
      "indices=tensor([104, 110, 105, 109, 108]))\n"
     ]
    }
   ],
   "source": [
    "#define a query \n",
    "query = \"explique la théorie de la normalisation\"\n",
    "print(f\"query: {query}\")\n",
    "\n",
    "embeded_query = embedding_model.encode(query, convert_to_tensor=True).to('cpu')\n",
    "embeded_query\n",
    "\n",
    "from time import perf_counter as timer \n",
    "\n",
    "dot_scores = util.dot_score(a=embeded_query.float(), b=embeddings.float())[0]\n",
    "\n",
    "top_values = torch.topk(dot_scores, k=5)\n",
    "\n",
    "print(top_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.7603526711463928\n",
      "text: Théorie de la normalisation  Cette théorie est basée sur les dépendances fonctionnelles qui permettent de  décomposer l'ensemble des informations en diverses relations. Chaque nouvelle forme  normale marque une étape dans la progression vers des relations présentant de moins en  moins de redondance. On applique les formes normales successivement à la relation  universelle afin d’obtenir un schéma normalisé.\n",
      "page:  93\n",
      "\n",
      "\n",
      "score:  0.5893518328666687\n",
      "text: Chapitre 3 : Le modèle Relationnel    99  La troisième forme normale ajoute une autre restriction à la seconde forme normale  en exprimant le fait que tous les attributs non clé dépendent complètement et uniquement  de la clé de la relation. La Boyce Codd forme normale pousse plus loin la restriction de la troisième forme  normale en exprimant le fait que dans toute relation en deuxième forme normale, s'il existe  une dépendance fonctionnelle alors sa partie gauche est forcément une clé de cette relation.   Figure 81 : Synthèse de la normalisation  7. Conception d'un schéma relationnel  Le processus de conception d'un schéma relationnel dont la qualité détermine la  pertinence et l'efficacité de la base de données, doit prendre en compte un certain ensemble  de critères dont les plus importants sont :  1. Normalité ;  2. Préservation des contenus ;  3. Préservation des dépendances fonctionnelles. A partir de l'ensemble des dépendances fonctionnelles, il est possible de construire  un bon schéma relationnel en exploitant des manipulations des dépendances fonctionnelles  (recherche de clé) et des algorithmes de conception de schéma relationnel. Il existe deux approches possibles pour concevoir un schéma relationnel, l'approche  par synthèse et l'approche par décomposition. 7. 1. Approche par synthèse  L'approche par synthèse consiste à recomposer des relations à partir d’un ensemble  d’attributs indépendants et de la couverture minimale d'un ensemble de dépendances  fonctionnelles. Pour cela on utilise l’algorithme de synthèse suivant :\n",
      "page:  99\n",
      "\n",
      "\n",
      "score:  0.5824511051177979\n",
      "text: Chapitre 3 : Le modèle Relationnel    94  Définition  La relation universelle est la relation qui regroupe toutes les informations à stocker (Tous les attributs  constatés). 6. 2. Pourquoi normaliser ? En modélisant les données dans une seule relation ou plusieurs relations ne  respectant pas les règles de normalisation, le schéma obtenu permettra un taux élevé de  redondances. L’objectif de la normalisation est de limiter cette redondance afin de permettre  de limiter les pertes de données ainsi que les incohérences entre elles. Et pour améliorer  également les performances de traitement.    Soit l’exemple suivant représentant la relation Enseignements :  Matricule  Nom  Module  Année  Niveau  01234555  DAHAK  BDD  2006/2007  3SI  01234555  DAHAK  BDD  2007/2008  3SI  01234555  DAHAK  BDD  2015/2016  1CS  01234555  DAHAK  IGL  2016/2017  1CS  On remarque que pour chaque ligne de la table, le matricule et le nom de l’enseignant  se répètent. D’un autre côté, en supprimant la dernière ligne on perd complètement  l’information qu’on dispose d’un module appelé IGL. Finalement, si on souhaite modifier le  matricule ou le nom de l’enseignant on sera obligé de le faire pour toutes les lignes sinon on  cause une incohérence au niveau des données. Ces problèmes apparaissent au niveau de cette relation parce qu’elle n’est pas  normalisée. L’objectif de la normalisation est donc de pallier à ce type de problèmes au niveau  des relations. 6. 3. Les formes normales  6. 3. 1. Première forme normale (1NF)  Définition  Une relation est en première forme normale si et seulement si tous ses attributs ont des valeurs simples (non  multiples, non composées)  La première forme normale est appliquée aux attributs de la relation en excluant les  attributs multiples et les attributs composés. Un attribut multiple est un attribut contenant  une liste de valeurs de même type comme la liste des enfants d’une personne.\n",
      "page:  94\n",
      "\n",
      "\n",
      "score:  0.5561892986297607\n",
      "text: Chapitre 3 : Le modèle Relationnel    98    Figure 79 : Relation ne respectant pas la BCNF  Soit la relation suivante Adresse(Rue, Commune, Bureau-Postal, CodePostal) avec  la DF problématique : CodePostal→Commune    Pour normalisation en Boyce-Codd forme normale il faut :  1. Isoler la DF problématique dans une nouvelle relation ;  2. Éliminer la cible de cette DF et la remplacer par sa source dans la relation initiale.   Figure 80 : Normalisation en BCNF  Remarque  Toute relation R en BCNF est forcément en 3 NF. En disant qu’une relation R est en BCNF ceci signifie que quel que soit X tq X→U,  X est une clé minimale. Supposons maintenant que R n’est pas en 3FN, ceci signifie qu’il  existe une DF transitive Y→Z tel que Y n’est pas une clé. Or cela contredit la définition de  la BCNF. Par contre, si R est en 3 NF elle n’est pas forcément en BCNF. 6. 3. 5. Synthèse  La première forme normale exprime le fait que toutes les valeurs des attributs sont  atomiques (simples, non composées). La seconde forme normale exprime le fait que tous les attributs non clé d'une relation  dépendent complètement de la clé de cette dernière et non pas d'une seule partie de cette  clé.\n",
      "page:  98\n",
      "\n",
      "\n",
      "score:  0.5499117970466614\n",
      "text: Chapitre 3 : Le modèle Relationnel    97    Figure 77 : Relation ne respectant pas la 3FN  Soit  la  relation  Enseignant(Num,  Nom,  Catégorie,  Classe,  Salaire)   avec la DF transitive : Catégorie, Classe→Salaire  Pour normaliser une relation en troisième forme normale il faut :  1. Isoler la DF transitive dans une nouvelle relation,  2. Éliminer la cible de la DF de la relation initiale.   Figure 78 : Normalisation en 3FN  La normalisation en 3FN de la relation Enseignant donne le schéma relationnel  suivant :  Enseignement(Num, Nom, Catégorie, Classe)  GrilleSalaire(Catégorie, Classe, Salaire)  Note  La 3FN exprime le fait que tous les attributs non clé dépendent complètement et uniquement de la clé de la  relation. 6. 3. 4. Forme normale de Boyce Codd(BCNF)  Définition   Une relation est en Boyce-Codd forme normale si et seulement si : elle est en deuxième forme normale et  toute source de dépendance fonctionnelle est une clé primaire minimale. La relation R(A, B, C, D, E) dont l’ensemble des DF est représenté par le graphe des  DF ci-dessous n'est pas en BCNF à cause de la DF : E→B.\n",
      "page:  97\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for value, index in zip(top_values.values.tolist(), top_values.indices.tolist()):\n",
    "    print(f\"score:  {value}\")\n",
    "    print(f\"text: {pages_and_chunks[index]['sentence_chunk']}\")\n",
    "    print(f\"page:  {pages_and_chunks[index]['page_number']}\")\n",
    "    print('\\n')\n",
    "    # print(f\"value : {value}\")\n",
    "    # print(f\"index : {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionazing the semantic search pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_semantic_search(query: str,\n",
    "                    embeddings: torch.tensor,\n",
    "                    top_n: int,\n",
    "                    model: SentenceTransformer= embedding_model):\n",
    "    \n",
    "    query_embeded = model.encode(query,convert_to_tensor=True)\n",
    "\n",
    "    #model encode return embeddings already normalized\n",
    "    dot_scores = util.dot_score(query_embeded.float(),embeddings.float())\n",
    "    top_scores_indexs = torch.topk(dot_scores, k=top_n)\n",
    "\n",
    "    return top_scores_indexs\n",
    "\n",
    "def top_scores_indexs(query: str,\n",
    "                      embeddings: torch.tensor,\n",
    "                      top_n: int,\n",
    "                      model: SentenceTransformer= embedding_model):\n",
    "\n",
    "    top_scores_indexs = retrieval_semantic_search(query,\n",
    "                          embeddings = embeddings,\n",
    "                          top_n=top_n)\n",
    "    return_list = []\n",
    "    #printing them\n",
    "    for value, index in zip(top_scores_indexs.values.tolist()[0], top_scores_indexs.indices.tolist()[0]):\n",
    "        values_map = {\n",
    "            \"score\": value,\n",
    "            \"text\": pages_and_chunks[index]['sentence_chunk'],\n",
    "            \"page\":  pages_and_chunks[index]['page_number']\n",
    "        }\n",
    "        return_list.append(values_map)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7603526711463928,\n",
       "  'text': \"Théorie de la normalisation  Cette théorie est basée sur les dépendances fonctionnelles qui permettent de  décomposer l'ensemble des informations en diverses relations. Chaque nouvelle forme  normale marque une étape dans la progression vers des relations présentant de moins en  moins de redondance. On applique les formes normales successivement à la relation  universelle afin d’obtenir un schéma normalisé.\",\n",
       "  'page': 93},\n",
       " {'score': 0.5893518328666687,\n",
       "  'text': \"Chapitre 3 : Le modèle Relationnel    99  La troisième forme normale ajoute une autre restriction à la seconde forme normale  en exprimant le fait que tous les attributs non clé dépendent complètement et uniquement  de la clé de la relation. La Boyce Codd forme normale pousse plus loin la restriction de la troisième forme  normale en exprimant le fait que dans toute relation en deuxième forme normale, s'il existe  une dépendance fonctionnelle alors sa partie gauche est forcément une clé de cette relation.   Figure 81 : Synthèse de la normalisation  7. Conception d'un schéma relationnel  Le processus de conception d'un schéma relationnel dont la qualité détermine la  pertinence et l'efficacité de la base de données, doit prendre en compte un certain ensemble  de critères dont les plus importants sont :  1. Normalité ;  2. Préservation des contenus ;  3. Préservation des dépendances fonctionnelles. A partir de l'ensemble des dépendances fonctionnelles, il est possible de construire  un bon schéma relationnel en exploitant des manipulations des dépendances fonctionnelles  (recherche de clé) et des algorithmes de conception de schéma relationnel. Il existe deux approches possibles pour concevoir un schéma relationnel, l'approche  par synthèse et l'approche par décomposition. 7. 1. Approche par synthèse  L'approche par synthèse consiste à recomposer des relations à partir d’un ensemble  d’attributs indépendants et de la couverture minimale d'un ensemble de dépendances  fonctionnelles. Pour cela on utilise l’algorithme de synthèse suivant :\",\n",
       "  'page': 99},\n",
       " {'score': 0.5824511051177979,\n",
       "  'text': 'Chapitre 3 : Le modèle Relationnel    94  Définition  La relation universelle est la relation qui regroupe toutes les informations à stocker (Tous les attributs  constatés). 6. 2. Pourquoi normaliser ? En modélisant les données dans une seule relation ou plusieurs relations ne  respectant pas les règles de normalisation, le schéma obtenu permettra un taux élevé de  redondances. L’objectif de la normalisation est de limiter cette redondance afin de permettre  de limiter les pertes de données ainsi que les incohérences entre elles. Et pour améliorer  également les performances de traitement.    Soit l’exemple suivant représentant la relation Enseignements :  Matricule  Nom  Module  Année  Niveau  01234555  DAHAK  BDD  2006/2007  3SI  01234555  DAHAK  BDD  2007/2008  3SI  01234555  DAHAK  BDD  2015/2016  1CS  01234555  DAHAK  IGL  2016/2017  1CS  On remarque que pour chaque ligne de la table, le matricule et le nom de l’enseignant  se répètent. D’un autre côté, en supprimant la dernière ligne on perd complètement  l’information qu’on dispose d’un module appelé IGL. Finalement, si on souhaite modifier le  matricule ou le nom de l’enseignant on sera obligé de le faire pour toutes les lignes sinon on  cause une incohérence au niveau des données. Ces problèmes apparaissent au niveau de cette relation parce qu’elle n’est pas  normalisée. L’objectif de la normalisation est donc de pallier à ce type de problèmes au niveau  des relations. 6. 3. Les formes normales  6. 3. 1. Première forme normale (1NF)  Définition  Une relation est en première forme normale si et seulement si tous ses attributs ont des valeurs simples (non  multiples, non composées)  La première forme normale est appliquée aux attributs de la relation en excluant les  attributs multiples et les attributs composés. Un attribut multiple est un attribut contenant  une liste de valeurs de même type comme la liste des enfants d’une personne.',\n",
       "  'page': 94},\n",
       " {'score': 0.5561892986297607,\n",
       "  'text': \"Chapitre 3 : Le modèle Relationnel    98    Figure 79 : Relation ne respectant pas la BCNF  Soit la relation suivante Adresse(Rue, Commune, Bureau-Postal, CodePostal) avec  la DF problématique : CodePostal→Commune    Pour normalisation en Boyce-Codd forme normale il faut :  1. Isoler la DF problématique dans une nouvelle relation ;  2. Éliminer la cible de cette DF et la remplacer par sa source dans la relation initiale.   Figure 80 : Normalisation en BCNF  Remarque  Toute relation R en BCNF est forcément en 3 NF. En disant qu’une relation R est en BCNF ceci signifie que quel que soit X tq X→U,  X est une clé minimale. Supposons maintenant que R n’est pas en 3FN, ceci signifie qu’il  existe une DF transitive Y→Z tel que Y n’est pas une clé. Or cela contredit la définition de  la BCNF. Par contre, si R est en 3 NF elle n’est pas forcément en BCNF. 6. 3. 5. Synthèse  La première forme normale exprime le fait que toutes les valeurs des attributs sont  atomiques (simples, non composées). La seconde forme normale exprime le fait que tous les attributs non clé d'une relation  dépendent complètement de la clé de cette dernière et non pas d'une seule partie de cette  clé.\",\n",
       "  'page': 98},\n",
       " {'score': 0.5499117970466614,\n",
       "  'text': \"Chapitre 3 : Le modèle Relationnel    97    Figure 77 : Relation ne respectant pas la 3FN  Soit  la  relation  Enseignant(Num,  Nom,  Catégorie,  Classe,  Salaire)   avec la DF transitive : Catégorie, Classe→Salaire  Pour normaliser une relation en troisième forme normale il faut :  1. Isoler la DF transitive dans une nouvelle relation,  2. Éliminer la cible de la DF de la relation initiale.   Figure 78 : Normalisation en 3FN  La normalisation en 3FN de la relation Enseignant donne le schéma relationnel  suivant :  Enseignement(Num, Nom, Catégorie, Classe)  GrilleSalaire(Catégorie, Classe, Salaire)  Note  La 3FN exprime le fait que tous les attributs non clé dépendent complètement et uniquement de la clé de la  relation. 6. 3. 4. Forme normale de Boyce Codd(BCNF)  Définition   Une relation est en Boyce-Codd forme normale si et seulement si : elle est en deuxième forme normale et  toute source de dépendance fonctionnelle est une clé primaire minimale. La relation R(A, B, C, D, E) dont l’ensemble des DF est représenté par le graphe des  DF ci-dessous n'est pas en BCNF à cause de la DF : E→B.\",\n",
       "  'page': 97}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"explique la théorie de la normalisation\"\n",
    "top_scores_indexs(query=query,\n",
    "                  embeddings = embeddings,\n",
    "                  top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6683e04f5a4a9fbb79e976b917fae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "#                                          bnb_4bit_compute_dtype=torch.float16)\n",
    "model_id = 'google/gemma-2b-it' \n",
    "print(f\"model_id: {model_id}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.bfloat16) # which attention version to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506172416"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 5012344832, 'model_mem_mb': 4780.14, 'model_mem_gb': 4.67}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) \n",
    "    model_mem_gb = model_mem_bytes / (1024**3) \n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "explique la théorie de la normalisation\n",
      "\n",
      "Prompt (formatted):\n",
      "<bos><start_of_turn>user\n",
      "explique la théorie de la normalisation<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"explique la théorie de la normalisation\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "#craete prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "#apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep it not tokenized\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 29s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inputs = tokenizer.encode(prompt, \n",
    "                      add_special_tokens= True,\n",
    "                      return_tensors=\"pt\" )\n",
    "\n",
    "outputs = llm_model.generate(input_ids=inputs.to(llm_model.device), max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     2,      2,    106,   1645,    108, 214032,    683, 117577,    581,\n",
       "           683,   4500,   5076,    107,    108,    106,   2516,    108,    688,\n",
       "          2841, 117577,    581,    683,   4500,   5076,    688,   1455,   2360,\n",
       "        117577, 160719,   2459,  22244,    581,   4500,   5975,   1437,  26173,\n",
       "          2173, 166333,  22424,   4255, 235265,  46531,  72828,    700, 235303,\n",
       "           477,   9273,   1437,  46895,    659,  26173, 166333,  22424,   4255,\n",
       "        235269,   2906,   2459, 121536,   1437,  16876, 128648,   1008,    683,\n",
       "        164526,   3633,    848,  26173, 166333,  22424,   4255, 235265,    109,\n",
       "           688,  13274,    667,  63245,  87995,    581,    683,   4500,   5076,\n",
       "           865,    688,    109, 235287,   5231,  83057,    581,    683,   8792,\n",
       "           865,    688,   2221,   8792,   4500,  47529,   1455,  55360,    547,\n",
       "          7077,    683,  39933,    581,    683,   8792,  38304,  36395,   2461,\n",
       "           755,   2360,  39933,  51391,   1008,   2360,  39933,   5039,  25760,\n",
       "        235265,    108, 235287,   5231,  83057,  30244,    865,    688,   2221,\n",
       "         39933,    581,    683,   8792,  62259,   1455,  15439,    619,    659,\n",
       "         14942,  37330,    683,  39933,    581,    683,   8792,  38304,    755,\n",
       "           683,  39933,  51391,   1008,    659,  55116,    741,    683,  39933,\n",
       "          5039,  25760, 235265,    109,    688,  35940,    581,    683,   4500,\n",
       "          5076,    865,    688,    109, 235287])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "explique la théorie de la normalisation<end_of_turn>\n",
      "<start_of_turn>model\n",
      "**La théorie de la normalisation** est une théorie statistique qui permet de normaliser les données non normalement distribuées. Cela signifie qu'on peut les transformer en données normalement distribuées, ce qui facilite les analyses statistiques et la comparaison avec des données normalement distribuées.\n",
      "\n",
      "**Principaux principes de la normalisation :**\n",
      "\n",
      "* **Transformation de la variable :** La variable normalisée est définie comme la valeur de la variable originale divisée par une valeur moyenne et une valeur standard deviation.\n",
      "* **Transformation inverse :** La valeur de la variable normale est obtenue en multipliant la valeur de la variable originale par la valeur moyenne et en ajoutant la valeur standard deviation.\n",
      "\n",
      "**Applications de la normalisation :**\n",
      "\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "#convert the output tokens into readable text\n",
    "output_decoded = tokenizer.decode(outputs[0])\n",
    "print(output_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>tokens_number</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1  Préambule  Après onze années en tant que ch...</td>\n",
       "      <td>94.00</td>\n",
       "      <td>[0.008394155651330948, -0.051100436598062515, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>J’ai alors fait des diapos pour chaque chapitr...</td>\n",
       "      <td>46.75</td>\n",
       "      <td>[-0.004756780341267586, -0.0710226446390152, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2  Pour finir, je pense avoir synthétisé ma mo...</td>\n",
       "      <td>12.75</td>\n",
       "      <td>[0.031454432755708694, 0.0174418855458498, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>13  Présentation du cours  Ce cours s’adresse ...</td>\n",
       "      <td>77.50</td>\n",
       "      <td>[-0.0037500502075999975, -0.030021093785762787...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>Théorie des ensembles,  3. Logique du premier ...</td>\n",
       "      <td>19.50</td>\n",
       "      <td>[-0.02807949110865593, 0.009503806941211224, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>145</td>\n",
       "      <td>145  Annexe 1 : Les Fonctions dans SQL  92  Do...</td>\n",
       "      <td>69.25</td>\n",
       "      <td>[-0.013162732124328613, -0.02656504325568676, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>146</td>\n",
       "      <td>146  IS [NOT] NULL  NULL  INNER JOIN  Jointure...</td>\n",
       "      <td>28.50</td>\n",
       "      <td>[-0.04927458614110947, -0.01684270054101944, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>147</td>\n",
       "      <td>147  Annexe 2 : Outils Pédagogiques  1. Functi...</td>\n",
       "      <td>45.25</td>\n",
       "      <td>[-0.01167414989322424, 0.0013812012039124966, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>147</td>\n",
       "      <td>Il est utilisé par les étudiants pour confront...</td>\n",
       "      <td>12.75</td>\n",
       "      <td>[-0.004473675042390823, -0.018602140247821808,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>148</td>\n",
       "      <td>148  2. Free Relational Algebra Interpreter  F...</td>\n",
       "      <td>47.75</td>\n",
       "      <td>[-0.03800264373421669, -0.00867730937898159, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page_number                                     sentence_chunk  \\\n",
       "0              1  1  Préambule  Après onze années en tant que ch...   \n",
       "1              1  J’ai alors fait des diapos pour chaque chapitr...   \n",
       "2              2  2  Pour finir, je pense avoir synthétisé ma mo...   \n",
       "3             13  13  Présentation du cours  Ce cours s’adresse ...   \n",
       "4             13  Théorie des ensembles,  3. Logique du premier ...   \n",
       "..           ...                                                ...   \n",
       "160          145  145  Annexe 1 : Les Fonctions dans SQL  92  Do...   \n",
       "161          146  146  IS [NOT] NULL  NULL  INNER JOIN  Jointure...   \n",
       "162          147  147  Annexe 2 : Outils Pédagogiques  1. Functi...   \n",
       "163          147  Il est utilisé par les étudiants pour confront...   \n",
       "164          148  148  2. Free Relational Algebra Interpreter  F...   \n",
       "\n",
       "     tokens_number                                         embeddings  \n",
       "0            94.00  [0.008394155651330948, -0.051100436598062515, ...  \n",
       "1            46.75  [-0.004756780341267586, -0.0710226446390152, -...  \n",
       "2            12.75  [0.031454432755708694, 0.0174418855458498, -0....  \n",
       "3            77.50  [-0.0037500502075999975, -0.030021093785762787...  \n",
       "4            19.50  [-0.02807949110865593, 0.009503806941211224, -...  \n",
       "..             ...                                                ...  \n",
       "160          69.25  [-0.013162732124328613, -0.02656504325568676, ...  \n",
       "161          28.50  [-0.04927458614110947, -0.01684270054101944, -...  \n",
       "162          45.25  [-0.01167414989322424, 0.0013812012039124966, ...  \n",
       "163          12.75  [-0.004473675042390823, -0.018602140247821808,...  \n",
       "164          47.75  [-0.03800264373421669, -0.00867730937898159, -...  \n",
       "\n",
       "[165 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "context_data = pd.read_csv('C:\\\\Users\\\\hp probook\\\\Desktop\\\\RAG_Project\\\\data_preprocessing\\\\bdd.csv')\n",
    "context_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Augmenting the prompt with the context items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_augment(query: str,\n",
    "                   context_items: list[dict]):\n",
    "    context = \"- \" + \"\\n- \".join([item['text'] for item in context_items])\n",
    "\n",
    "    base_prompt =\"\"\" if the question isn't in the field and the scope of database or there is no context items attached with the question then don't answer it, otherwise based on these context items, please answer the question:\n",
    "    context_items: {context}\n",
    "    query: {query}\n",
    "    Answer: \n",
    "    \"\"\"    \n",
    "    base_prompt = base_prompt.format(context=context,\n",
    "                                query = query)\n",
    "    # input_text = context + \"\\n\\n\" + query\n",
    "    \n",
    "    dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": base_prompt}\n",
    "    ]\n",
    "    \n",
    "    #apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                           tokenize=False, # keep it not tokenized\n",
    "                                           add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"explique la théorie de la normalisation\"\n",
    "context = top_scores_indexs(query=query,\n",
    "                          embeddings = embeddings,\n",
    "                          top_n=5)\n",
    "prompt = prompt_augment(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp probook\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 33s\n",
      "Wall time: 6min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs = tokenizer.encode(prompt, \n",
    "                      add_special_tokens= True,\n",
    "                      return_tensors=\"pt\" )\n",
    "\n",
    "outputs = llm_model.generate(input_ids=inputs,\n",
    "                            temperature = 0.7,\n",
    "                            max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: explique la théorie de la normalisation\n",
      "Answer:\n",
      "<bos> **La théorie de la normalisation** est une approche pour améliorer les performances des bases de données en minimisant les redundances et en garantissant la validité des données.\n",
      "\n",
      "**Les trois formes normales** sont :\n",
      "\n",
      "1. **Première forme normale (1NF)** : une relation est en 1NF si tous les attributs sont de type simple (non-multiples et non-composés).\n",
      "2. **Deuxième forme normale (2NF)** : une relation est en 2NF si toutes les relations non clés sont dépendantes de la clé.\n",
      "3. **Troisième forme normale (3NF)** : une relation est en 3NF si toutes les relations non clés sont dépendantes de la clé et que la clé est minimale (une clé minimale est une clé qui ne contient aucune autre clé comme dépendance).\n",
      "\n",
      "**La normalisation** vise à atteindre une relation en 3NF en isolant les dépendances fonctionnelles de la clé et en exprimant ces dépendances de manière plus concise. Cela permet de réduire les redundances et de garantir la validité des données.<eos>\n"
     ]
    }
   ],
   "source": [
    "#convert the output tokens into readable text\n",
    "output_decoded = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer:\\n{output_decoded.replace(prompt,' ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionize all the process from thge query to the answer generation:\n",
    "\n",
    "- takes a query\n",
    "- find relevent contexts \n",
    "- augment the prompt\n",
    "- tokenize it\n",
    "- generate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_dahak(query: str, \n",
    "              temperature: float=0.7, \n",
    "              max_new_token: int=256,\n",
    "              format_answer_text = True,\n",
    "              answer_only = True\n",
    "               ):\n",
    "\n",
    "    context = top_scores_indexs(query=query,\n",
    "                                embeddings = embeddings,\n",
    "                                top_n=5)\n",
    "    \n",
    "    prompt = prompt_augment(query=query, context_items=context)\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt, \n",
    "                          add_special_tokens= True,\n",
    "                          return_tensors=\"pt\" )\n",
    "    \n",
    "    outputs = llm_model.generate(input_ids=inputs,\n",
    "                                temperature = temperature,\n",
    "                                max_new_tokens=max_new_token)\n",
    "\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    # format the answer \n",
    "    if format_answer_text:\n",
    "        # replace the tokens and remove the prompt\n",
    "        output_text = output_text.replace(prompt,\" \").replace(\"<bos>\", \" \").replace(\"<eos>\",\" \")\n",
    "    \n",
    "    if answer_only:\n",
    "        return output_text #without the context items\n",
    "    \n",
    "    \n",
    "    return output_text, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: explique l'entité faible et donne moi exemple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp probook\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:   **Entité faible**\n",
      "\n",
      "Une entité faible est une entité possédant un identifiant insuffisant de par lui-même pour identifier de manière unique chacune de ses occurrences. Sa caractéristique d’identifiant n’est valable qu’à l’intérieur du contexte spécifique de l’occurrence d’une entité principale.\n",
      "\n",
      "**Exemple**\n",
      "\n",
      "Dans notre exemple, l'identifiant d'une chambre est constitué de deux parties : l'identifiant de l'hôtel et le numéro de la chambre. \n",
      "\n",
      "Context: [{'score': 0.5391520857810974, 'text': ' On peut dire finalement, qu’une entité est une réalisation ou concrétisation des  propriétés d’un type-entité ce qui signifie qu’on donne une valeur à chaque attribut du type- entité pour obtenir une entité de ce type-entité.', 'page': 24}, {'score': 0.4777226746082306, 'text': ' La relation est représentée sous une forme tabulaire dont chaque ligne représente une  extension.', 'page': 79}, {'score': 0.46703875064849854, 'text': \"Chapitre 2 : Conception des bases de données avec le modèle Entité Association    23  2. 1. 1. Entité  Définition  Une entité est un objet, une chose concrète ou abstraite qui peut être reconnue distinctement et qui est  caractérisée par son unicité. Dès qu’on rencontre des objets identifiables de manière unique (sans confusion ni  ambiguïté) qui interagissent et qui font ou subissent des actions dans notre système on parle  d’entités.  Prenons l’exemple suivant :   Ali lit le livre intitulé « Bases de données » qu’il a acheté chez Amar.     Figure 2 : Exemple d'une situation réelle  On distingue nettement trois entités : Ali, le livre « Bases de données » et Amar. Ali  fait deux actions, celles de lire et d’acheter. Le livre « Bases de données » subit ces mêmes  actions alors que Amar participe à l’action d’achat du livre par Ali qui est interprétée de son  coté comme étant une action de vente. Les entités concrètes sont des entités que l'on peut voir et toucher, leur présence est  physique. Alors que les entités abstraites sont présentes dans notre esprit, on peut les  ressentir et connaitre leur rôle dans une situation donnée mais n'ont aucune forme physique.  Quand on parle par exemple de la situation d'un enseignant qui enseigne un cours à  des étudiants. Les enseignants et les étudiants sont des objets concrets et le cours est un objet  abstrait. 2. 1. 2. Type - Entité  Lors de la modélisation d'une situation donnée, on ne peut gérer toutes les entités  présentes ni les représenter dans le modèle. On est donc amené à passer à un niveau  d'abstraction plus élevé. On regroupe ainsi toutes les entités de même nature afin d'en avoir\", 'page': 23}, {'score': 0.4604705274105072, 'text': \"Chapitre 2 : Conception des bases de données avec le modèle Entité Association    42  Définition  Une entité faible est une entité possédant un identifiant insuffisant de par lui-même pour identifier de  manière unique chacune de ses occurrences. Sa caractéristique d’identifiant n’est valable qu’à l’intérieur du  contexte spécifique de l’occurrence d’une entité principale. Dans le cas de l'exemple précédent, l’identifiant d’une chambre est constitué de deux  parties : l'identifiant de l'hôtel et le numéro de la chambre. OBSERVATION  L’introduction d’entités faibles n’est pas une nécessité absolue puisqu’on peut très bien utiliser une  association classique. La principale différence est que, dans le cas d’une entité faible, on obtient une  identification composée qui est souvent plus pratique à gérer, et peut également rendre plus faciles certaines  requêtes. Alors que dans le second cas, on doit créer nous même un identifiant fictif  qu’il faudra gérer par la  suite. La cardinalité du côté de l’entité faible est toujours 1, 1. La cardinalité maximale 1 se justifie par  le fait d’identification composée (Chaque entité du type-entité faible n’a qu’une seule entité de l’autre type- entité par rapport à laquelle elle s’identifie) et la cardinalité minimale ne peut pas être 0 car cela signifierai  qu’une entité du type-entité faible pourrait exister sans participer à l’association ce qui est contradictoire avec  la définition de l’entité faible    Peut-on définir des entités faibles en cascade ? Il est possible d’avoir des entités faibles en cascade, l’identifiant des entités est obtenu  dans ce cas en effectuant la migration des identifiants des pères vers les fils, en cascade  également. Considérons l'exemple suivant :  Dans notre école les niveaux d'études sont identifiés par rapport au cycle : {Cycle  Commun, Cycle Supérieur}, nous avons 2 niveaux du premier cycle et trois niveaux du  second cycle. Dans chaque cycle on peut trouver plusieurs sections qui sont identifiées par  des lettres de l'alphabet {Section A, Section B etc. }. Les groupes sont identifiés par un  numéro séquentiel dans le niveau {Groupe 1, Groupe 2 etc. } On peut donc trouver plusieurs groupes avec le numéro 1 mais dans des niveaux  différents, et plusieurs sections A mais également dans des niveaux différents. Idem pour les  niveaux, on a le niveau 1 du cycle CC et le niveau 1 du cycle CS.  Pour modéliser cela, on peut utiliser des associations classiques avec des cardinalités  1, 1 et gérer des identifiants avec des valeurs composées.\", 'page': 42}, {'score': 0.45638442039489746, 'text': 'Chapitre 2 : Conception des bases de données avec le modèle Entité Association    24  une seule représentation générale ou générique. C’est analogue au concept d’ensemble dans  les mathématiques en considérant les entités comme les éléments de l’ensemble. Cette représentation généralisée des entités est appelée type-entité et définie comme  suit :  Définition   Un type-entité désigne un ensemble d’entités qui possèdent une sémantique et des propriétés communes. Dans l’exemple précédent, on a présenté une situation bien définie, celle de Ali qui  lit le livre « Bases de données » qu’il a acheté chez Amar. Mais si on désire modéliser cette  situation de manière générale, On fera en sorte que notre modèle puisse représenter cette  situation quel que soit le lecteur, l’acheteur, le livre et le vendeur. Pour cela on ne devrait plus  parler de Ali, du livre « Bases de données » et de Amar précisément mais de ce qu’ils  représentent, à savoir : l’acheteur, le livre et le vendeur. Donc on pourrait dire finalement que  Ali et Amar sont des entités du type-entité Personne car tous deux ont les mêmes  caractéristiques et le livre « Bases de données » est une entité du type-entité Livre. Au niveau du modèle EA, ce sont les types-entités qui sont représentés et non pas  les entités. Une entité est souvent nommée occurrence ou instance de son type-entité.  Par abus de langage, on utilise souvent le mot entité au lieu du mot type-entité, il faut  cependant prendre garde à ne pas confondre les deux concepts. 2. 2. Attribut et Valeur  2. 2. 1. Attribut  Définition   Un attribut (propriété) est une caractéristique associée à un type-entité ou à un type-association. Dans l’exemple précédent, Ali et Amar sont des prénoms des deux entités qu’ils  désignent et « Bases de données » est le titre du livre acheté par Ali. En fait « Ali » est une  valeur de la propriété prénom, c’est donc le prénom qui est un attribut et non pas Ali, idem  pour le titre du livre.', 'page': 24}]\n"
     ]
    }
   ],
   "source": [
    "query = \"explique l'entité faible et donne moi exemple\"\n",
    "print(f\"Query: {query}\")\n",
    "answer, context = ask_dahak(query, answer_only=False)\n",
    "\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "print(f\"\\nContext: {context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
